K Means Tutorial: http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/index.html

K-Means 的算法如下：
1. 随机在图中取 K(这里 K = 2)个种子点.
2. 然后对图中的所有点求到这 K 个种子点的距离, 假如点 Pi 离种子点 Si 最近, 那么 Pi 属于 Si 点群.
3. 接下来, 我们要移动种子点到属于他的“点群”的中心.
4. 然后重复 第 2)和第 3)步, 直到, 种子点没有移动.

求点群中心的算法:
可以很简的使用各个点的 X/Y 坐标的平均值.
Minkowski Distance 公式 —— λ 可以随意取值, 可以是负数, 也可以是正数, 或是无穷大.
Euclidean Distance 公式 —— 也就是第一个公式 λ=2 的情况
CityBlock Distance 公式 —— 也就是第一个公式 λ=1 的情况

这三个公式的求中心点有一些不一样的地方, 他们是怎么个逼近中心的, 第一个以星形的方式, 第二个以同心圆的方式, 第三个以菱形的方式.

K-Means++ 算法
K-Means 主要有两个最重大的缺陷——都和初始值有关：
1. K 是事先给定的, 这个 K 值的选定是非常难以估计的. 很多时候, 事先并不知道给定的数据集应该分成多少个类别才最合适. (ISODATA 算法通过类的自动合并和分裂, 得到较为合理的类型数目 K)
2. K-Means 算法需要用初始随机种子点, 这个随机种子点太重要, 不同的随机种子点会有得到完全不同的结果. (K-Means++ 算法可以用来解决这个问题, 其可以有效地选择初始点)

K-Means++ 算法步骤：
1. 先从数据库随机挑个随机点当“种子点”. 
2. 对于每个点, 都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里, 然后把这些距离加起来得到 Sum(D(x)). 
3. 然后, 再取一个随机值, 用权重的方式来取计算下一个“种子点”. 这个算法的实现是, 先取一个能落在 Sum(D(x))中的随机值 Random, 然后用 Random -= D(x), 直到其 <=0, 此时的点就是下一个“种子点”. 
4. 重复第 2) 和第 3) 步直到所有的 K 个种子点都被选出来.
5. 进行 K-Means 算法.


KNN
K Nearest Neighbor 算法又叫 KNN 算法, 这个算法是机器学习里面一个比较经典的算法. 其中的 K 表示最接近自己的 K 个数据样本. 
KNN 算法和 K-Means 算法不同的是, K-Means 算法用来聚类, 用来判断哪些东西是一个比较相近的类型, 而 KNN 算法是用来做归类的, 
也就是说, 有一个样本空间里的样本分成很几个类型, 然后, 给定一个待分类的数据, 通过计算接近自己最近的 K 个样本来判断这个待分类数据属于哪个分类. 
你可以简单的理解为由那离自己最近的 K 个点来投票决定待分类数据归为哪一类. 

机器学习, 算法基本上都比较简单, 最难的是数学建模, 把那些业务中的特性抽象成向量的过程, 另一个是选取适合模型的数据样本. 这两个事都不是简单的事. 算法反而是比较简单的事. 
对于 KNN 算法中找到离自己最近的 K 个点, 是一个很经典的算法面试题, 需要使用到的数据结构是“最大堆——Max Heap”, 一种二叉树. 


