    聚类分析中的 "类" (cluster) 和分类的 "类" (class) 是不同的, 对 cluster 更加准确的翻译应该是 "簇".
聚类的任务是把所有的实例分配到若干的簇, 使得同一个簇的实例聚集在一个簇中心的周围, 它们之间距离的比较近; 而不同簇实例之间的距离比较远.
对于由数值型属性刻画的实例来说, 这个距离通常指欧氏距离.
    分类作为一种监督学习方法, 要求必须明确知道各类别的信息, 并且断言所有待分类项都有一个类别与之对应.
但很多时候上述条件得不到满足, 尤其是在处理海量数据的时候, 如果通过预处理使得数据满足分类算法的要求, 则代价非常大,
这时候可以考虑聚类算法. 聚类算法属于无监督学习, 相比于分类, 聚类不依赖预定义的类和类标号的训练实例.
	根据聚成的簇的特点, 聚类技术通常分为层次聚类(hierarchical clustering)和划分聚类(partitional clustering).
前者比较典型的例子是凝聚层次聚类算法, 后者的典型例子是 K-Means 算法.

特征选择(feature selection)
近邻测度(proximity measure)

Hierarchical Clustering and Dendrograms
层次聚类和聚类分析

Single-Link and Complete-Link Clustering
单链和完整链聚类

Single linkage(nearest neighbor): 两个 cluster 中最近的对象的距离为 cluster 之间的距离
Complete linkage (furthest neighbor)：两个 cluster 中最远的对象的距离为 cluster 之间的距离
Group average linkage: 两个 cluster 中对象的平均距离为 cluster 之间的距离

Single-link Clustering
	chops off longest link
Clustering with proximity bounds
	Merges based on proximity
Extract for K-clusters
	You can specify how many clusters you want
Complete-Link Clustering
	var of single link using a whole cluster
Within-Cluster Point Scatter
	You don’t need to specify the number of clusters.
	It detects the best breaking point.
	This is the method used to do NER across documents.

层次聚类算法可分为凝聚(agglomerative, 自底向上)和分裂(divisive, 自顶向下)两种, 算法简单, 而且相比 flat clustering 没有预设聚类个数的问题, 并且树状的输出也比较直观

K-Means 均值聚类法分为如下几个步骤:
一、初始化聚类中心
	1、根据具体问题, 凭经验从样本集中选出 C 个比较合适的样本作为初始聚类中心
	2、用前 C 个样本作为初始聚类中心
	3、将全部样本随机地分成 C 类, 计算每类的样本均值, 将样本均值作为初始聚类中心
二、初始聚类
	1、按就近原则将样本归入各聚类中心所代表的类中
	2、取一样本, 将其归入与其最近的聚类中心的那一类中, 重新计算样本均值, 更新聚类中心. 然后取下一样本, 重复操作, 直至所有样本归入相应类中
三、判断聚类是否合理
	采用误差平方和准则函数判断聚类是否合理, 不合理则修改分类. 循环进行判断、修改直至达到算法终止条件

K-Modes 聚类算法
	K-Modes 是在数据挖掘中对分类属性型数据采用的算法, 是对 K-Means 算法的扩展.
	K-Means(K 均值) 是在数据挖掘领域中普遍应用的算法, 它只能处理数值型数据, 而不能处理分类属性型数据.
	例如表示人的属性有: 姓名、性别、年龄、家庭住址等属性. 而 K-Modes 就能够处理分类属性型数据.
	K-Modes 采用差异度来代替 K-Means 中的距离. K-Modes 中差异度越小, 则表示距离越小.
	一个样本和一个聚类中心的差异度就是它们各个属性不相同的个数, 不相同则记为一, 最后计算一的总和.
	这个和就是某个样本到某个聚类中心的差异度. 该样本属于差异度最小的聚类中心.

Latent Dirichlet allocation (隐含狄利克雷分配)
    一个基于概率分布模型, 主要用于处理离散的数据集合, 目前主要用在数据挖掘(DM)中的 text mining 和自然语言处理中, 主要是用来降低维度的.
LDA 是文本建模的一种方法, 它属于生成模型. 生成模型是指该模型可以随机生成可观测的数据, LDA 可以随机生成一篇由 N 个主题组成文章.
通过对文本的建模, 可以对文本进行主题分类, 判断相似度等. 在 90 年代提出的 LSA 中, 通过对向量空间进行降维, 获得文本的潜在语义空间.
在 LDA 中则是通过将文本映射到主题空间, 即认为一个文章有若干主题随机组成, 从而获得文本间的关系.
LDA 模型有一个前提: bag of word. 意思就是认为文档就是一个词的集合, 忽略任何语法或者出现顺序关系.

生成模型
	LDA 的建模过程是逆向通过文本集合建立生成模型, 在讨论如何建模时, 先要理解 LDA 的生成模型如何生成一篇文档.
假设一个语料库中有三个主题: 体育, 科技, 电影
	一篇描述电影制作过程的文档, 可能同时包含主题科技和主题电影, 而主题科技中有一系列的词, 这些词和科技有关,
并且他们有一个概率, 代表的是在主题为科技的文章中该词出现的概率. 同理在主题电影中也有一系列和电影有关的词,
并对应一个出现概率. 当生成一篇关于电影制作的文档时, 首先随机选择某一主题, 选择到科技和电影两主题的概率更高;
然后选择单词, 选择到那些和主题相关的词的概率更高. 这样就就完成了一个单词的选择. 不断选择 N 个单词, 这样就组成了一篇文档.

线性判别式分析(Linear Discriminant Analysis, LDA)
	也叫做 Fisher 线性判别(Fisher Linear Discriminant, FLD), 是模式识别的经典算法, 它是在 1996 年由 Belhumeur 引入模式识别和人工智能领域的.
性鉴别分析的基本思想是将高维的模式样本投影到最佳鉴别矢量空间, 以达到抽取分类信息和压缩特征空间维数的效果,
投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离, 即模式在该空间中有最佳的可分离性.因此, 它是一种有效的特征抽取方法.
使用这种方法能够使投影后模式样本的类间散布矩阵最大, 并且同时类内散布矩阵最小.
就是说, 它能够保证投影后模式样本在新的空间中有最小的类内距离和最大的类间距离, 即模式在该空间中有最佳的可分离性.

B-Cubed
一种基于谱聚类的共指消解方法

聚类算法的种类:
基于划分聚类算法(partition clustering)
K-Means:		一种典型的划分聚类算法, 它用一个聚类的中心来代表一个簇, 即在迭代过程中选择的聚点不一定是聚类中的一个点, 该算法只能处理数值型数据
K-Modes:		K-Means 的扩展, 采用简单匹配方法来度量分类型数据的相似度
K-Prototypes:	结合了 K-Means 和 K-Modes 两种算法, 能够处理混合型数据
K-Medoids:		在迭代过程中选择簇中的某点作为聚点, PAM 是典型的 K-Medoids 算法
CLARA:			CLARA 算法在 PAM 的基础上采用了抽样技术, 能够处理大规模数据
CLARANS:		CLARANS 算法融合了 PAM 和 CLARA 两者的优点, 是第一个用于空间数据库的聚类算法
Focused CLARAN: 采用了空间索引技术提高了 CLARANS 算法的效率
PCM:			模糊集合理论引入聚类分析中并提出了 PCM 模糊聚类算法

基于层次聚类算法:
CURE:	采用抽样技术先对数据集 D 随机抽取样本, 再采用分区技术对样本进行分区, 然后对每个分区局部聚类, 最后对局部聚类进行全局聚类
ROCK:	也采用了随机抽样技术, 该算法在计算两个对象的相似度时, 同时考虑了周围对象的影响
CHEMALOEN(变色龙): 首先由数据集构造成一个 K-最近邻图 Gk, 再通过一个图的划分算法将图 Gk 划分成大量的子图, 每个子图代表一个初始子簇, 最后用一个凝聚的层次聚类算法反复合并子簇, 找到真正的结果簇
SBAC:	SBAC 则在计算对象间相似度时, 考虑了属性特征对于体现对象本质的重要程度, 对于更能体现对象本质的属性赋予较高的权值
BIRCH:	利用树结构对数据集进行处理, 叶结点存储一个聚类, 用中心和半径表示, 顺序处理每一个对象, 并把它划分到距离最近的结点, 该算法也可以作为其他聚类算法的预处理过程
BUBBLE:		把 BIRCH 算法的中心和半径概念推广到普通的距离空间
BUBBLE-FM: 	通过减少距离的计算次数, 提高了 BUBBLE 算法的效率

基于密度聚类算法:
DBSCAN:		典型的基于密度的聚类算法, 采用空间索引技术来搜索对象的邻域, 引入了“核心对象”和“密度可达”等概念, 从核心对象出发, 把所有密度可达的对象组成一个簇
GDBSCAN:	通过泛化 DBSCAN 算法中邻域的概念, 以适应空间对象的特点
DBLASD:
OPTICS:		结合了聚类的自动性和交互性, 先生成聚类的次序, 可以对不同的聚类设置不同的参数, 来得到用户满意的结果
FDC:		通过构造 k-d tree 把整个数据空间划分成若干个矩形空间, 当空间维数较少时可以大大提高 DBSCAN 的效率

基于网格的聚类算法:
STING:			利用网格单元保存数据统计信息, 从而实现多分辨率的聚类
WaveCluster:	在聚类分析中引入了小波变换的原理, 主要应用于信号处理领域. (备注: 小波算法在信号处理, 图形图像, 加密解密等领域有重要应用, 是一种比较高深和牛逼的东西)
CLIQUE:			一种结合了网格和密度的聚类算法
OPTIGRID:

基于神经网络的聚类算法:
自组织神经网络 SOM:	该方法的基本思想是--由外界输入不同的样本到人工的自组织映射网络中, 一开始时, 输入样本引起输出兴奋细胞的位置各不相同, 但自组织后会形成一些细胞群, 它们分别代表了输入样本, 反映了输入样本的特征

基于统计学的聚类算法:
COBWeb:		COBWeb 是一个通用的概念聚类方法, 它用分类树的形式表现层次聚类
CLASSIT:
AutoClass:	以概率混合模型为基础, 利用属性的概率分布来描述聚类, 该方法能够处理混合型的数据, 但要求各属性相互独立

从以上对传统的聚类分析方法所做的总结来看, 不管是 K-Means 方法, 还是 CURE 方法, 在进行聚类之前都需要用户事先确定要得到的聚类的数目.
然而在现实数据中, 聚类的数目是未知的, 通常要经过不断的实验来获得合适的聚类数目, 得到较好的聚类结果.
聚类分析是一个富有挑战性的研究领域, 以下就是对数据挖掘中聚类分析的一些典型要求:
(1) 可伸缩性(scalability)
	实际应用要求聚类算法能够处理大数据集, 且时间复杂度不能太高(最好是多项式时间), 消耗的内存空间也有限.
	目前, 为了将算法拓展到超大数据库(VLDB)领域, 研究人员已经进行了许多有益的尝试,
	包括：增量式挖掘、可靠的采样、数据挤压(data squashing)等.
	其中, 数据挤压技术首先通过扫描数据来获得数据的统计信息, 然后在这些统计信息的基础上进行聚类分析. 比如 BIRCH 算法中使用 CF 树就是属于数据挤压技术.
(2) 能够处理不同类型的属性
	现实中的数据对象己远远超出关系型数据的范畴, 比如空间数据、多媒体数据、遗传学数据、时间序列数据、文本数据、万维网上的数据、以及目前逐渐兴起的数据流.
	这些数据对象的属性类型往往是由多种数据类型综合而成的.
(3) 能够发现任意形状的簇
(4) 尽量减少用于决定输入参数的领域知识
(5) 能够处理噪声数据及孤立点
(6) 对输入数据记录的顺序不敏感
(7) 高维性(high-dimensional)
	一个数据集可能包含若干维. 较高的维数给聚类分析带来两个问题:
	首先, 不相关的属性削弱了数据汇聚的趋势, 使得数据分布非常稀疏. 尽管这种情况在低维空间中并不多见, 但是随着维数的增加, 不相关属性的出现概率及数量也会增加, 最后导致数据空间中几乎不存在簇.
	其次, 高维使得在低维中很有效的区分数据的标准在高维空间中失效了.
	如在高维空间中, 数据点到最近邻点的距离与到其他点的距离没有多少分别, 从而导致最近邻查询在高维空间中不稳定, 此时若根据接近度来划分簇, 结果是不可信的.
(8) 能够根据用户指定的约束条件进行聚类
(9) 聚类结果具有可解释性和可用性

